{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UpQVm6iH6RDOKjU-fl5gTfDsc80fOOxZ","timestamp":1627397450310}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D9UUcbMe5uHd"},"source":["# - LAB4 : VGG, RESNET model simple Implementation with CIFAR10 dataset (45 min)\n","#### LAB4 에서는 CNN 아키텍쳐 중 가장 대표적인 VGG과 RESNET 모델을 간단히 구현해보고 CIFAR-10 데이터셋에 대해 학습 및 Inference를 해볼 계획입니다."]},{"cell_type":"markdown","metadata":{"id":"pMY3CPV4ebV2"},"source":["## Example 1) VGG16 모델 구현\n","- [doc] (https://arxiv.org/pdf/1409.1556.pdf)\n","\n","![image.png](http://drive.google.com/uc?id=1E6MVIcFCsImwWQGOhC-8KUQqAe2W_I28)\n","\n","![image.png](http://drive.google.com/uc?id=1jT9jhqMzEaHoma5xvro5jf6PFOq7FlLJ)\n","\n","![image.png](http://drive.google.com/uc?id=17DgT11woHwXACEGOfvkVd8pYmxRI7vQl)"]},{"cell_type":"code","metadata":{"id":"jbce4wfMMTqv"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, utils\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","\n","# DEVICE 설정\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","# Parameter 설정\n","EPOCHS = 10\n","BATCH_SIZE = 64\n","LR = 0.0001\n","\n","# Transform 설정\n","transform_CIFAR10 = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","])\n","\n","# Dataset 설정\n","train_dataset = datasets.CIFAR10(root = '../data',\n","                                         train = True,\n","                                         download = True,\n","                                         transform = transform_CIFAR10)\n","\n","test_dataset = datasets.CIFAR10(root = '../data',\n","                                train = False,\n","                                download = True,\n","                                transform = transform_CIFAR10)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                           batch_size = BATCH_SIZE,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                          batch_size = BATCH_SIZE,\n","                                          shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JeDJmfNuN0k"},"source":["# Model 구현\n","class Custom_VGG(nn.Module):\n","    def __init__(self):\n","        super(Custom_VGG, self).__init__()\n","        ########################################## Complete This Code~!\n","\n","        ########################################## Complete This Code~!\n","\n","    def forward(self, x):\n","        ########################################## Complete This Code~!\n","\n","        ########################################## Complete This Code~!\n","        return x\n","model = Custom_VGG().to(DEVICE)\n","summary(model, (3,32,32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jkqHEqDBaoPQ"},"source":["from torchvision import models\n","model_import = models.vgg16(pretrained=False, num_classes=10).to(DEVICE)\n","summary(model_import, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_b5IsHnKUquV"},"source":["# Optimizer 설정\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oy8oXE_mWARx"},"source":["# Train 구현\n","def train_one_epoch(model, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            \n","# Evaluation 구현\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","\n","            # 배치 오차를 합산\n","            test_loss += F.cross_entropy(output, target,\n","                                         reduction='sum').item()\n","\n","            # 가장 높은 값을 가진 인덱스가 바로 예측값\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_one_epoch(model, train_loader, optimizer, epoch)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n","          epoch, test_loss, test_accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNxz2ARrE9Jp"},"source":["## Excercise 1) RESNET18 모델 구현\n","- [doc] (https://arxiv.org/pdf/1512.03385.pdf)\n","\n","![image.png](http://drive.google.com/uc?id=1GgHATI5PFF8-PlBdGp9vDra2maRqLybl)\n","\n","![image.png](http://drive.google.com/uc?id=1EYxIKJEI0rwIyW7ZZaFeZgJvol6Jb-XL)\n","\n","![image.png](http://drive.google.com/uc?id=17DgT11woHwXACEGOfvkVd8pYmxRI7vQl)\n","\n","![image.png](http://drive.google.com/uc?id=12reHf9xtapZrVBG4LlNbNGa37ZeFfUqk)"]},{"cell_type":"code","metadata":{"id":"5_IJRFtgHw02"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, utils\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","\n","# DEVICE 설정\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","# Parameter 설정\n","EPOCHS = 10\n","BATCH_SIZE = 64\n","LR = 0.01\n","\n","# Transform 설정\n","transform_CIFAR10 = transforms.Compose([\n","    # transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","])\n","\n","# Dataset 설정\n","train_dataset = datasets.CIFAR10(root = '../data',\n","                                         train = True,\n","                                         download = True,\n","                                         transform = transform_CIFAR10)\n","\n","test_dataset = datasets.CIFAR10(root = '../data',\n","                                train = False,\n","                                download = True,\n","                                transform = transform_CIFAR10)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                           batch_size = BATCH_SIZE,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                          batch_size = BATCH_SIZE,\n","                                          shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1dykk5Om-cPG"},"source":["# Model 구현\n","class Custom_RESNET(nn.Module):\n","    def __init__(self):\n","        super(Custom_RESNET, self).__init__()\n","        self.maxpool2d = nn.MaxPool2d(kernel_size=3, stride=2)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_features=64)\n","        \n","        self.conv2_1 = nn.Conv2d(64,64,3,padding=1)\n","        self.conv2_2 = nn.Conv2d(64,64,3,padding=1)\n","        self.conv2_3 = nn.Conv2d(64,64,3,padding=1)\n","        self.conv2_4 = nn.Conv2d(64,64,3,padding=1)\n","        self.bn2_1 = nn.BatchNorm2d(num_features=64)\n","        self.bn2_2 = nn.BatchNorm2d(num_features=64)\n","        self.bn2_3 = nn.BatchNorm2d(num_features=64)\n","        self.bn2_4 = nn.BatchNorm2d(num_features=64)\n","\n","        self.conv3_1 = nn.Conv2d(64,128,3,padding=1, stride=2)\n","        self.conv3_2 = nn.Conv2d(128,128,3,padding=1)\n","        self.conv3_3 = nn.Conv2d(128,128,3,padding=1)\n","        self.conv3_4 = nn.Conv2d(128,128,3,padding=1)\n","        self.bn3_1 = nn.BatchNorm2d(num_features=128)\n","        self.bn3_2 = nn.BatchNorm2d(num_features=128)\n","        self.bn3_3 = nn.BatchNorm2d(num_features=128)\n","        self.bn3_4 = nn.BatchNorm2d(num_features=128)\n","\n","        self.conv4_1 = nn.Conv2d(128,256,3,padding=1, stride=2)\n","        self.conv4_2 = nn.Conv2d(256,256,3,padding=1)\n","        self.conv4_3 = nn.Conv2d(256,256,3,padding=1)\n","        self.conv4_4 = nn.Conv2d(256,256,3,padding=1)\n","        self.bn4_1 = nn.BatchNorm2d(num_features=256)\n","        self.bn4_2 = nn.BatchNorm2d(num_features=256)\n","        self.bn4_3 = nn.BatchNorm2d(num_features=256)\n","        self.bn4_4 = nn.BatchNorm2d(num_features=256)\n","\n","        self.conv5_1 = nn.Conv2d(256,512,3,padding=1, stride=2)\n","        self.conv5_2 = nn.Conv2d(512,512,3,padding=1)\n","        self.conv5_3 = nn.Conv2d(512,512,3,padding=1)\n","        self.conv5_4 = nn.Conv2d(512,512,3,padding=1)\n","        self.bn5_1 = nn.BatchNorm2d(num_features=512)\n","        self.bn5_2 = nn.BatchNorm2d(num_features=512)\n","        self.bn5_3 = nn.BatchNorm2d(num_features=512)\n","        self.bn5_4 = nn.BatchNorm2d(num_features=512)\n","\n","        self.adaptiveavgpool2d = nn.AdaptiveAvgPool2d(1)\n","\n","        self.fc = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        ########################################## Complete This Code~!\n","\n","        ########################################## Complete This Code~!\n","\n","        x = self.adaptiveavgpool2d(x5_4)\n","        x = x.view(-1,512)\n","        x = self.fc(x)\n","        return x\n","\n","model = Custom_RESNET().to(DEVICE)\n","summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJq3fJZHHw7_"},"source":["from torchvision import models\n","model_import = models.resnet18(pretrained=False, num_classes=10).to(DEVICE)\n","summary(model_import, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUam0mfLavpg"},"source":["# Model, Optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GM0Tm7RJHzv-"},"source":["# Train 구현\n","def train_one_epoch(model, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(DEVICE), target.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            \n","# Evaluation 구현\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            output = model(data)\n","\n","            # 배치 오차를 합산\n","            test_loss += F.cross_entropy(output, target,\n","                                         reduction='sum').item()\n","\n","            # 가장 높은 값을 가진 인덱스가 바로 예측값\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_one_epoch(model, train_loader, optimizer, epoch)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n","          epoch, test_loss, test_accuracy))"],"execution_count":null,"outputs":[]}]}